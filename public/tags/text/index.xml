<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Text on Oras Al-Kubaisi</title>
    <link>http://localhost:1313/tags/text/</link>
    <description>Recent content in Text on Oras Al-Kubaisi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/text/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to detect AI generated text - Analysing n-grams</title>
      <link>http://localhost:1313/posts/detecting-ai-generated-text-ngrams/</link>
      <pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/detecting-ai-generated-text-ngrams/</guid>
      <description>Detecting AI generated text is becoming more important than ever and the use cases are endless. First examples come to mind is blog articles, academic papers, school assignments and so on.&#xA;There are many companies now offering services to detect AI content, and I got curious on how these companies are doing it. In the AI era, the fastest solution would be using multiple LLMs to identify if the text is AI generated or not, but how we can determine the accuracy?</description>
    </item>
  </channel>
</rss>
